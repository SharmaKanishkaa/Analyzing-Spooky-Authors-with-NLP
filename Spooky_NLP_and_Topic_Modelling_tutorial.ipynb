{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7516,
          "databundleVersionId": 44045,
          "sourceType": "competition"
        },
        {
          "sourceId": 3586,
          "sourceType": "datasetVersion",
          "datasetId": 2134
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Spooky NLP and Topic Modelling tutorial",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharmaKanishkaa/Analyzing-Spooky-Authors-with-NLP/blob/main/Spooky_NLP_and_Topic_Modelling_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "![](https://png.pngtree.com/thumb_back/fw800/background/20230615/pngtree-halloween-cuties-wallpaper-2017-image_2947813.jpg)\n",
        "\n",
        "In this notebook, I shall conduct a very basic attempt at topic modelling this Spooky Author dataset. Topic modelling is the process in which we try uncover abstract themes or \"topics\" based on the underlying documents and words in a corpus of text. I will introduce two standard topic modelling techniques here with the first technique known as Latent Dirichlet Allocation (LDA) and the second Non-negative Matrix Factorization (NMF). I will also take the opportunity to introduce some Natural Language Processing basics such as Tokenization, Stemming and vectorization of the raw text which should also hopefully come in handy when making predictions with learning models.\n",
        "\n",
        "The outline of this notebook is as follows:\n",
        "\n",
        "1. **Exploratory Data Analysis (EDA) and Wordclouds** - Analyzing the data by generating simple statistics such word frequencies over the different authors as well as plotting some wordclouds (with image masks).\n",
        "\n",
        "\n",
        "2. **Natural Language Processing (NLP) with NLTK (Natural Language Toolkit) ** - Introducing basic text processing methods such as tokenizations, stop word removal, stemming and vectorizing text via term frequencies (TF) as well as the inverse document frequencies (TF-IDF)\n",
        "\n",
        "3. **Topic Modelling with LDA and NNMF** - Implementing the two topic modelling techniques of Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF).\n",
        "\n",
        "So, are you brave enough to take the next step and continue reading through this ghastly notebook?\n",
        "\n",
        "---\n",
        "\n",
        "**\"*Because they pop! Pop, pop! Pop, pop! Pop, pop, pop!*\" **- Pennywise, The Dancing Clown"
      ],
      "metadata": {
        "_uuid": "075ab0f3fc310e293828b3681f1d80642f88c106",
        "_cell_guid": "a3cb0ee3-7bca-4b2b-8a27-be198d18818e",
        "id": "OP6Z0TBwJJKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plotly imports\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "\n",
        "# Other imports\n",
        "from collections import Counter\n",
        "import imageio\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:24.410674Z",
          "iopub.execute_input": "2023-09-11T13:29:24.410996Z",
          "iopub.status.idle": "2023-09-11T13:29:24.586618Z",
          "shell.execute_reply.started": "2023-09-11T13:29:24.410973Z",
          "shell.execute_reply": "2023-09-11T13:29:24.585511Z"
        },
        "trusted": true,
        "id": "WfpH046cJJKR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the training data with Pandas\n",
        "train = pd.read_csv(\"../input/spooky-author-identification/train.zip\", compression='zip', sep=',', quotechar='\"')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:24.588228Z",
          "iopub.execute_input": "2023-09-11T13:29:24.588634Z",
          "iopub.status.idle": "2023-09-11T13:29:24.700499Z",
          "shell.execute_reply.started": "2023-09-11T13:29:24.588611Z",
          "shell.execute_reply": "2023-09-11T13:29:24.699817Z"
        },
        "trusted": true,
        "id": "aua7dOrVJJKT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "_uuid": "0060aa157588b45653d3cb2b9c03bad3ff36900e",
        "_cell_guid": "0e41fb5c-e937-45db-b0ba-743017896b1a",
        "id": "BvCV2WanJJKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. The Authors and their works EDA\n",
        "\n",
        "First step, let us take a look at a quick peek of what the first three rows in the data has in store for us and who exactly are the authors"
      ],
      "metadata": {
        "_uuid": "97ee6de5cf262e0dcff70fb9bc74466b1ffe1bd8",
        "_cell_guid": "624cc24b-893e-470d-83b1-2a3f585f6394",
        "id": "momQJCWsJJKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:24.70146Z",
          "iopub.execute_input": "2023-09-11T13:29:24.701853Z",
          "iopub.status.idle": "2023-09-11T13:29:24.719986Z",
          "shell.execute_reply.started": "2023-09-11T13:29:24.701832Z",
          "shell.execute_reply": "2023-09-11T13:29:24.718933Z"
        },
        "trusted": true,
        "id": "7hjJzAf1JJKY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the competition page there are three distinct author initials we have already been provided with a mapping of these initials to the actual author which is as follows:\n",
        "\n",
        "*(Links to their Wikipedia page profiles if you click on their names)*\n",
        "\n",
        "1. **[EAP - Edgar Allen Poe](https://en.wikipedia.org/wiki/Edgar_Allan_Poe)** : American writer who wrote poetry and short stories that revolved around tales of mystery and the grisly and the grim. Arguably his most famous work is the poem - \"The Raven\" and he is also widely considered the pioneer of the genre of the detective fiction.\n",
        "\n",
        "2. **[HPL - HP Lovecraft](https://en.wikipedia.org/wiki/H._P._Lovecraft)** : Best known for authoring works of horror fiction, the stories that he is most celebrated for revolve around the fictional mythology of the infamous creature \"Cthulhu\" - a hybrid chimera mix of Octopus head and humanoid body with wings on the back.\n",
        "\n",
        "3. **[MWS - Mary Shelley](https://en.wikipedia.org/wiki/Mary_Shelley)** : Seemed to have been involved in a whole panoply of literary pursuits - novelist, dramatist, travel-writer, biographer. She is most celebrated for the classic tale of Frankenstein where the scientist Frankenstein a.k.a \"The Modern Prometheus\" creates the Monster that comes to be associated with his name.\n",
        "\n",
        "Next, let us take a look at how large the training data is:"
      ],
      "metadata": {
        "_uuid": "aa3c93c40e62d21d3762651b35c25edcb5d3d746",
        "_cell_guid": "178ee8ea-a97e-49bf-be63-38396ee454b2",
        "id": "V_rK-oRHJJKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:24.723383Z",
          "iopub.execute_input": "2023-09-11T13:29:24.723756Z",
          "iopub.status.idle": "2023-09-11T13:29:24.731888Z",
          "shell.execute_reply.started": "2023-09-11T13:29:24.723729Z",
          "shell.execute_reply": "2023-09-11T13:29:24.730574Z"
        },
        "trusted": true,
        "id": "cmOUUXIBJJKb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary statistics of the training set\n",
        "\n",
        "Here we can visualize some basic statistics in the data, like the distribution of entries for each author. For this purpose, I will invoke the handy Plot.ly visualisation library and plot some simple bar plots. Unhide the cell below if you want to see the Plot.ly code."
      ],
      "metadata": {
        "_uuid": "e90aef77086b604ed50757bb54a74e37d92a2a1e",
        "_cell_guid": "cf0a6e25-b953-40df-be64-f0316e420954",
        "id": "Zs_cbHgPJJKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\n",
        "data = [go.Bar(x = train.author.map(z).unique(),\n",
        "               y = train.author.value_counts().values,\n",
        "               marker = dict(colorscale='Jet',\n",
        "                            color = train.author.value_counts().values),\n",
        "               text = 'Text entries attributed to Author')]\n",
        "\n",
        "layout = go.Layout(title='Target variable distribution')\n",
        "\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "\n",
        "py.iplot(fig, filename='basic-car')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:24.733273Z",
          "iopub.execute_input": "2023-09-11T13:29:24.733613Z",
          "iopub.status.idle": "2023-09-11T13:29:24.827287Z",
          "shell.execute_reply.started": "2023-09-11T13:29:24.733586Z",
          "shell.execute_reply": "2023-09-11T13:29:24.825886Z"
        },
        "trusted": true,
        "id": "JYxpu-z8JJKd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = train['text'].str.split(expand=True).unstack().value_counts()\n",
        "data = [go.Bar(x=all_words.index.values[0:50],\n",
        "               y=all_words.values[0:50],\n",
        "               marker=dict(colorscale='Jet',\n",
        "                           color=all_words.values[0:100]\n",
        "                          ),\n",
        "               text='word counts'\n",
        ")]\n",
        "\n",
        "layout = go.Layout(title = 'Top 50 (Uncleaned) word frecuencies in the training set')\n",
        "\n",
        "fig = go.Figure(data = data, layout = layout)\n",
        "\n",
        "py.iplot(fig, filename='basic-bar')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:24.828959Z",
          "iopub.execute_input": "2023-09-11T13:29:24.829208Z",
          "iopub.status.idle": "2023-09-11T13:29:26.361675Z",
          "shell.execute_reply.started": "2023-09-11T13:29:24.829187Z",
          "shell.execute_reply": "2023-09-11T13:29:26.360522Z"
        },
        "trusted": true,
        "id": "sH1j1ONvJJKe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice anything odd about the words that appear in this word frequency plot? Do these words actually tell us much about the themes and concepts that Mary Shelley wants to portray to the reader in her stories?\n",
        "\n",
        "These words are all so commonly occuring words which you could find just anywhere else. Not just in spooky stories and novels by our three authors but also in newspapers, kid book, religious texts - really almost every other english text. Therefore we must find some way to preprocess our dataset first to strip out all these commonly occurring words which do not bring much to the table."
      ],
      "metadata": {
        "_uuid": "58dd795e28474ff99c36b3196ee1767dcf206eed",
        "_cell_guid": "311422ae-29c8-4694-9041-aa5c1601b8a0",
        "id": "S1pjXWW2JJKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordClouds to visualise each author's work\n",
        "\n",
        "One very handy visualization tool for a data scientist when it comes to any sort of natural language processing is plotting \"Word Cloud\". A word cloud (as the name suggests) is an image that is made up of a mixture of distinct words which may make up a text or book and where the size of each word is proportional to its word frequency in that text (number of times the word appears). Here instead of dealing with an actual book or text, our words can simply be taken from the column \"text\"\n",
        "\n",
        "**Store the text of each author in  a Python list**\n",
        "\n",
        "We first create three different python lists that store the texts of Edgar Allen Poe, HP Lovecraft and Mary Shelley respectively as follows:"
      ],
      "metadata": {
        "_uuid": "fe3ad42b8d423aafe9190490f180db59c0669e8a",
        "_cell_guid": "bbfe7efe-74a4-45ad-bfba-d926ed9819d4",
        "id": "ImTfdT5uJJKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eap = train[train.author=='EAP']['text'].values\n",
        "hpl = train[train.author=='HPL']['text'].values\n",
        "mws = train[train.author=='MWS']['text'].values"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:26.362909Z",
          "iopub.execute_input": "2023-09-11T13:29:26.363499Z",
          "iopub.status.idle": "2023-09-11T13:29:26.377551Z",
          "shell.execute_reply.started": "2023-09-11T13:29:26.363451Z",
          "shell.execute_reply": "2023-09-11T13:29:26.376797Z"
        },
        "trusted": true,
        "id": "ZQYuw0KRJJKg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next to create our wordclouds, I will import the python module \"wordcloud\"."
      ],
      "metadata": {
        "_uuid": "e8efa4d45bc38f9524b079c9eec497969c2e0f91",
        "_cell_guid": "a27e06c7-d219-40be-b2b4-0d982fb9b47a",
        "id": "h25j2L3hJJKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:26.385313Z",
          "iopub.execute_input": "2023-09-11T13:29:26.385924Z",
          "iopub.status.idle": "2023-09-11T13:29:26.39063Z",
          "shell.execute_reply.started": "2023-09-11T13:29:26.385894Z",
          "shell.execute_reply": "2023-09-11T13:29:26.389592Z"
        },
        "trusted": true,
        "id": "QqcXIXWZJJKh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "But generating a normal wordcloud is rather boring so I would like to introduce to you a technique of importing pictures (something relevant) and using the outline of that picture as a mask for our wordclouds. Therefore the pictures that I have chosen are the ones I feel most representative for their authors:\n",
        "\n",
        "1. ) The Raven for Edgar Allen Poe 2.) Octopus Cthulu-thingy for HP Lovecraft and 3.) Frankenstein for Mary Shelly\n",
        "\n",
        "The way I am loading in the pictures on Kaggle is a sort of a feature hack although readers familiar to my work know this trick. I first derive the Base64 encoding of whatever images I want to use and then use that particular encoding and re-convert the picture back on the notebook. The cell below contains the Base64 encoding of the three images I will use but I have hidden them so that I do not pollute this notebook with just long streteches of text - unhide them if you want to see the encoding."
      ],
      "metadata": {
        "_uuid": "236f9d322896163101b09c5b4f612ad4ebfa0971",
        "_cell_guid": "a698e137-5526-46cc-8188-38f9d05b5844",
        "id": "1R5yacROJJKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To decode the image, I import the \"codecs\" module and save it as a new image here in this notebook. Once the image is saved, I can simply load it as a mask as follows:"
      ],
      "metadata": {
        "_uuid": "c66a004440ddd98f95d30f24659a7ab684b998fc",
        "_cell_guid": "245cc1f1-f0bf-4c06-85e1-d6cf2fb40d52",
        "id": "afgXjnHuJJKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally plotting the word clouds via the following few lines (unhide to see the code):"
      ],
      "metadata": {
        "_uuid": "87f658344ed7a57ac0de56ae1c3a682e579e0c99",
        "_cell_guid": "05937068-0a5e-4c6b-b40a-7a330032a2cb",
        "id": "-64_-tJYJJKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edgar Allen Poe wordcloud\n",
        "plt.figure(figsize=(20,18))\n",
        "wc = WordCloud(background_color='black', max_words=10000, stopwords=STOPWORDS, max_font_size=40)\n",
        "wc.generate(\" \".join(eap))\n",
        "plt.title('Edgar Allen Poe', fontsize=20)\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:26.39196Z",
          "iopub.execute_input": "2023-09-11T13:29:26.392284Z",
          "iopub.status.idle": "2023-09-11T13:29:27.708426Z",
          "shell.execute_reply.started": "2023-09-11T13:29:26.392257Z",
          "shell.execute_reply": "2023-09-11T13:29:27.70767Z"
        },
        "trusted": true,
        "id": "G6x3SOu2JJKi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# HP Lovecraft wordcloud\n",
        "plt.figure(figsize=(20,18))\n",
        "wc = WordCloud(background_color='black', max_words=10000, stopwords=STOPWORDS, max_font_size=40)\n",
        "wc.generate(\" \".join(hpl))\n",
        "plt.title('HP Lovecraft')\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:27.711925Z",
          "iopub.execute_input": "2023-09-11T13:29:27.712224Z",
          "iopub.status.idle": "2023-09-11T13:29:28.900863Z",
          "shell.execute_reply.started": "2023-09-11T13:29:27.712198Z",
          "shell.execute_reply": "2023-09-11T13:29:28.899536Z"
        },
        "trusted": true,
        "id": "jK0YvH95JJKi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Mary Shelley wordcloud\n",
        "plt.figure(figsize=(20,18))\n",
        "wc = WordCloud(background_color='black', stopwords=STOPWORDS, max_words=10000, max_font_size=40)\n",
        "wc.generate(\" \".join(mws))\n",
        "plt.title('Mary Shelley')\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:28.904053Z",
          "iopub.execute_input": "2023-09-11T13:29:28.904305Z",
          "iopub.status.idle": "2023-09-11T13:29:29.99098Z",
          "shell.execute_reply.started": "2023-09-11T13:29:28.904284Z",
          "shell.execute_reply": "2023-09-11T13:29:29.989877Z"
        },
        "trusted": true,
        "id": "9KIdzFISJJKi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well there you have it, three separate word clouds, one for each of our spooky authors. Right off the bat, you can see from these word clouds some of the choice words that were favoured by the different authors.\n",
        "\n",
        "For example, you can see the HP lovecraft favours words like \"dream\", \"time\", \"strange\", \"past\", \"ancient\" which seem to resonate with themes that the author was famous for, themes around the hidden psyche and esoteric nature of fate and chance as well as the infamous creature Cthulhu and mentions of ancient cults and rituals associated with it.\n",
        "\n",
        "On the other hand, one can see that Mary Shelley's words revolve around primal instincts and themes of morality which range from the positive to negative ends of the spectrum, such as \"friend\", \"fear\", \"hope\", \"spirit\" etc. - themes which resonate in her works such as Frankenstein\n",
        "\n",
        "However, as you can see from the word clouds, there are still a handful of words that seem to be quite out of place. Words such as \"us\", \"go\", \"he\" which seem to appear commonly every where in text"
      ],
      "metadata": {
        "_uuid": "a026400479d03567e4c6455b7d90215d2ff2f8cf",
        "_cell_guid": "d67381d1-d765-449e-a9b9-6e3b127580aa",
        "id": "dJtIp2udJJKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "_uuid": "dfdb5bcd1cb2cf9275fa53e233dd87298817a366",
        "_cell_guid": "263dc0b2-0991-4e63-81d3-3c9f77279cfb",
        "id": "GInmkljmJJKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Natural Language Processing\n",
        "\n",
        "![](https://s-media-cache-ak0.pinimg.com/originals/d8/ca/04/d8ca049fd1ad2d92818763e55c57f49a.jpg)\n",
        "\n",
        "In almost all Natural Language Processing (the field that explores interactions between a computer and human languages) tasks that you will come across (be it topic modelling, or word clustering or document-text classification etc), one will generally always have to undergo these few pre-processing steps to convert the input raw text into a form that is readable by your model and the machine. You certainly can't expect to feed a Random Forest model a paragraph of words and expect it to immediately predict which author that paragraph came from. Behind the scenes, text pre-processing can be boiled down to these few simple steps:\n",
        "\n",
        "1. **Tokenization** - Segregation of the text into its individual constitutent words.\n",
        "2. **Stopwords** - Throw away any words that occur too frequently as its frequency of occurrence will not be useful in helping detecting relevant texts. (as an aside also consider throwing away words that occur very infrequently).\n",
        "3. **Stemming**  - combine variants of words into a single parent word that still conveys the same meaning\n",
        "4. **Vectorization** - Converting text into vector format. One of the simplest is the famous bag-of-words approach, where you create a matrix (for each document or text in the corpus). In the simplest form, this matrix stores word frequencies (word counts) and is often referred to as vectorization of the raw text.\n",
        "\n",
        "**Natural Language Toolkit (NLTK)**: To make our Natural Language Processing endeavours more convenient, let me introduce to you one of the most handy toolkits that on NLP - the Natural Language Toolkit, also more commonly referred to as the  [NLTK](http://www.nltk.org/) module. To import the toolkit, it is as easier as:\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "245126697b14d9e76cb43924c71ec02a1ad76ebf",
        "_cell_guid": "9dc7e717-a262-48c1-8684-b0fca2398d35",
        "id": "Lrv2lv4eJJKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:29.992727Z",
          "iopub.execute_input": "2023-09-11T13:29:29.993178Z",
          "iopub.status.idle": "2023-09-11T13:29:29.998066Z",
          "shell.execute_reply.started": "2023-09-11T13:29:29.993152Z",
          "shell.execute_reply": "2023-09-11T13:29:29.996888Z"
        },
        "trusted": true,
        "id": "Frr064CUJJKk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2a. Tokenization\n",
        "\n",
        "The concept of tokenization is the act of taking a sequence of characters (think of Python strings) in a given document and dicing it up into its individual constituent pieces, which are the eponymous \"tokens\" of this method. One could loosely think of them as singular words in a sentence. One could naively implement the \"split( )\" method on a string which separates it into a python list based on the identifier in the argument. It is actually not that trivial to\n",
        "\n",
        "Here we split the first sentence of the text in the training data just on a space as follows:"
      ],
      "metadata": {
        "_uuid": "3d3ac7816eee311d2016a48bb681f3fb6556bddf",
        "_cell_guid": "6a9618ba-2ec0-4a6f-8169-060b7bed5427",
        "id": "Rbh1AaDvJJKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the first text element as a string\n",
        "first_text = train.text.values[0]\n",
        "print(first_text)\n",
        "print('='*90)\n",
        "print(first_text.split(' '))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:29.999722Z",
          "iopub.execute_input": "2023-09-11T13:29:30.000038Z",
          "iopub.status.idle": "2023-09-11T13:29:30.013416Z",
          "shell.execute_reply.started": "2023-09-11T13:29:30.000014Z",
          "shell.execute_reply": "2023-09-11T13:29:30.011831Z"
        },
        "trusted": true,
        "id": "LZ2sqwmyJJKk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "However as you can see from this first attempt at tokenization, the segregation of the sentence into its individual elements (or terms) is not entirely accurate. As an example, look at the second element of the list which contains the term \"process,\". The punctuation mark (comma) has also been included and is being treated along with the word \"process\" as a term in itself. Ideally we would like the comma and the word to be in two different and separate elements of the list. Trying to do this with pure python list operations will be quite complex so this is where the NLTK library comes into play. There is a convenient method \"word_tokenize( )\" (TreebankWord tokenizer) which strips out singular words as well as punctuations into separate elements automatically as follows:"
      ],
      "metadata": {
        "_uuid": "25df56d59982058eba6ccc54953936f138a5fe0e",
        "_cell_guid": "f265ba84-773b-4488-b056-830d0dfe0ca3",
        "id": "H0vKxbaAJJKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_text_list = nltk.word_tokenize(first_text)\n",
        "print(first_text_list)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:30.01482Z",
          "iopub.execute_input": "2023-09-11T13:29:30.015091Z",
          "iopub.status.idle": "2023-09-11T13:29:30.033774Z",
          "shell.execute_reply.started": "2023-09-11T13:29:30.015066Z",
          "shell.execute_reply": "2023-09-11T13:29:30.032867Z"
        },
        "trusted": true,
        "id": "1wGgD_AEJJKl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2b. Stopword Removal\n",
        "\n",
        "As alluded to above stopwords are generally words that appear so commonly and at such a high frequency in the corpus that they don't actually contribute much to the learning or predictive process as a learning model would fail to distinguish it from other texts. Stopwords include terms such as \"to\" or \"the\" and therefore, it would be to our benefit to remove them during the pre-processing phase. Conveniently, NLTK comes with a predefined list of 153 english stopwords. (unhide the second cell below to see this list)"
      ],
      "metadata": {
        "_uuid": "35c495ac87055092c021facb767b107c5568ec4d",
        "_cell_guid": "061e26ac-280f-4399-9b33-6a3cde26ea3b",
        "id": "GHWCbzfMJJKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "len(stopwords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:30.035513Z",
          "iopub.execute_input": "2023-09-11T13:29:30.036152Z",
          "iopub.status.idle": "2023-09-11T13:29:30.04586Z",
          "shell.execute_reply.started": "2023-09-11T13:29:30.036118Z",
          "shell.execute_reply": "2023-09-11T13:29:30.044836Z"
        },
        "trusted": true,
        "id": "Tv9Ht_hfJJKm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:30.047245Z",
          "iopub.execute_input": "2023-09-11T13:29:30.047675Z",
          "iopub.status.idle": "2023-09-11T13:29:30.053358Z",
          "shell.execute_reply.started": "2023-09-11T13:29:30.047647Z",
          "shell.execute_reply": "2023-09-11T13:29:30.052665Z"
        },
        "trusted": true,
        "id": "zPi4pOI6JJKm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To filter out stop words from our tokenized list of words, we can simply use a list comprehension as follows:"
      ],
      "metadata": {
        "_uuid": "dc35cb71dd9f6fb43d4fc4c160e1638af6f2f276",
        "_cell_guid": "e54202f3-b39d-497b-9612-5f6f50529036",
        "id": "DFcazAWUJJKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_text_list_cleaned = [word for word in first_text_list if word.lower() not in stopwords]\n",
        "print(first_text_list_cleaned)\n",
        "print('='*90)\n",
        "print('Length of the original list: {0} words\\n'\n",
        "     'Length of the list after stopwords removal: {1} words'\n",
        "     .format(len(first_text_list), len(first_text_list_cleaned)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:30.054344Z",
          "iopub.execute_input": "2023-09-11T13:29:30.054605Z",
          "iopub.status.idle": "2023-09-11T13:29:30.066823Z",
          "shell.execute_reply.started": "2023-09-11T13:29:30.054583Z",
          "shell.execute_reply": "2023-09-11T13:29:30.065547Z"
        },
        "trusted": true,
        "id": "JbzB4W_XJJKn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, our list with stopwords removed is now substantially shorter than before where commonly occuring words such as \"I\", \"me\", \"to\" and \"the\" have been removed."
      ],
      "metadata": {
        "_uuid": "be431b48073ed91c54920d4883edd3c8b597d0ae",
        "_cell_guid": "c980fcb1-5cdc-4061-b94d-47ab6b860144",
        "id": "v8wbhG8iJJKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2c. Stemming and Lemmatization\n",
        "\n",
        "After removal of stopwords, the next stage of NLP that I would like to introduce is the process of Stemming.  The work at this stage attempts to reduce as many different variations of similar words into a single term ( different branches all reduced to single word *stem*). Therefore if we have \"running\", \"runs\" and \"run\", you would really want these three distinct words to collapse into just the word \"run\". (However of course you lose granularity of the past, present or future tense).\n",
        "\n",
        "We can turn to NLTK again which provides various stemmers which include variants such as the Porter stemming algorithm, the lancaster stemmer and the Snowball stemmer. In the following example, I will create a porter stemmer instance as follows:"
      ],
      "metadata": {
        "_uuid": "265c9f86933af289e043337db3462bd02845c6c1",
        "_cell_guid": "7495e09a-e8b6-4e7b-bdbd-cd75a3ce27a6",
        "id": "RpmdJgSyJJKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = nltk.stem.PorterStemmer()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:30.067986Z",
          "iopub.execute_input": "2023-09-11T13:29:30.068335Z",
          "iopub.status.idle": "2023-09-11T13:29:30.076256Z",
          "shell.execute_reply.started": "2023-09-11T13:29:30.068305Z",
          "shell.execute_reply": "2023-09-11T13:29:30.075371Z"
        },
        "trusted": true,
        "id": "rkQsN8DkJJKw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can use stemmer to see if it can reduce our these test words (\"running\", \"runs\", \"run\") into their a single stemmed word. Conveniently we can test the stemmer on the fly as follows:"
      ],
      "metadata": {
        "_uuid": "09e18504e483a896b0484baf92a46b3f7c014ea9",
        "_cell_guid": "b33b3723-64b6-4996-9d27-bacf3c8c7211",
        "id": "nWfldwYCJJKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The stemmed form of running is: {}'.format(stemmer.stem('running')))\n",
        "print('The stemmed form of runs is: {}'.format(stemmer.stem('runs')))\n",
        "print('The stemmed form of run is: {}'.format(stemmer.stem('run')))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:30.077299Z",
          "iopub.execute_input": "2023-09-11T13:29:30.077639Z",
          "iopub.status.idle": "2023-09-11T13:29:30.089252Z",
          "shell.execute_reply.started": "2023-09-11T13:29:30.077613Z",
          "shell.execute_reply": "2023-09-11T13:29:30.088324Z"
        },
        "trusted": true,
        "id": "Kw48-w-LJJKx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the stemmer has successfully reduced the given words above into a base form and this will be most in helping us reduce the size of our dataset of words when we come to learning and classification tasks.\n",
        "\n",
        "However there is one flaw with stemming and that is the fact that the process involves quite a [crude heuristic in chopping off the ends of words](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) in the hope of reducing a particular word into a human recognizable base form. Therefore this process does not take into account vocabulary or word forms when collapsing words as this example will illustrate:"
      ],
      "metadata": {
        "_uuid": "1634de190898f3232bc53d741f7fc6f7207c9438",
        "_cell_guid": "33b0cd45-db7e-4309-be16-b509d1dd9073",
        "id": "Wv-t0b_nJJKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The stemmed form of leaves is: {}'.format(stemmer.stem('leaves')))"
      ],
      "metadata": {
        "_uuid": "09944cdd9990bfc060adeb33fb839e38bebd1951",
        "_cell_guid": "8812bd89-c2d9-4916-9e47-db51ae8c939a",
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:30.090785Z",
          "iopub.execute_input": "2023-09-11T13:29:30.091018Z",
          "iopub.status.idle": "2023-09-11T13:29:30.101451Z",
          "shell.execute_reply.started": "2023-09-11T13:29:30.090998Z",
          "shell.execute_reply": "2023-09-11T13:29:30.099942Z"
        },
        "trusted": true,
        "id": "6Qk_kBdIJJKy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization to the rescue\n",
        "\n",
        "Therefore we turn to another that we could use in lieu of stemming. This method is called lemmatization which aims to achieve the same effect as the former method. However unlike a stemmer, lemmatizing the dataset aims to reduce words based on an actual dictionary or vocabulary (the Lemma) and therefore will not chop off words into stemmed forms that do not carry any lexical meaning. Here we can utilize NLTK once again to initialize a lemmatizer (WordNet variant) and inspect how it collapses words as follows:"
      ],
      "metadata": {
        "_uuid": "9ea2b6d17255ff5a421213b58722a0c8174facbd",
        "_cell_guid": "a051ac61-9948-44f1-b5fc-112c77a851c8",
        "id": "U24QHYbcJJKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Needed unzip in the current enviroment***"
      ],
      "metadata": {
        "id": "Z7v8X3AlJJKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
      ],
      "metadata": {
        "_uuid": "fd50d5b556e74060707dcc26dd2599b179db4f31",
        "_cell_guid": "afdc99cc-a30d-40ba-8cba-a4f72637fd6e",
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:30.104248Z",
          "iopub.execute_input": "2023-09-11T13:29:30.104589Z",
          "iopub.status.idle": "2023-09-11T13:29:30.650116Z",
          "shell.execute_reply.started": "2023-09-11T13:29:30.104555Z",
          "shell.execute_reply": "2023-09-11T13:29:30.649338Z"
        },
        "trusted": true,
        "id": "OJAxl89ZJJKz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemm = WordNetLemmatizer()\n",
        "print('The lemmatized form of leaves is: {}'.format(lemm.lemmatize('leaves')))"
      ],
      "metadata": {
        "_uuid": "fd50d5b556e74060707dcc26dd2599b179db4f31",
        "_cell_guid": "afdc99cc-a30d-40ba-8cba-a4f72637fd6e",
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:30.651092Z",
          "iopub.execute_input": "2023-09-11T13:29:30.65136Z",
          "iopub.status.idle": "2023-09-11T13:29:32.174839Z",
          "shell.execute_reply.started": "2023-09-11T13:29:30.651337Z",
          "shell.execute_reply": "2023-09-11T13:29:32.173956Z"
        },
        "trusted": true,
        "id": "Xi0NHO4kJJK0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brilliant, now we see that our lemmatizer is working and collapsing words into a form that makes much more lexical sense."
      ],
      "metadata": {
        "_uuid": "5b4c03ed72e67882014cfcbfca7339f649a02c97",
        "_cell_guid": "849d2968-5a45-4aaa-b5c2-e985e5e08016",
        "id": "OV1tBrc8JJK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2d. Vectorizing Raw Text\n",
        "\n",
        "In the vast collection of NLP literature, there are many different purposes for analyzing raw text, where in some cases you would like to compare the similarity of one body of text to another (Clustering techniques/Distance measurements), text classification (the purpose of this competition) as well as uncovering the topics that comprise a body of text (the aim of this notebook). With the purpose of uncovering topics at the back of our minds we must now think of how to feed the raw text into a machine learning model. Having already discussed tokenization, stopword removals and stemming (or maybe lemmatizing) we have now arrived at a reasonably cleaner text dataset then we started out with. However at this juncture, our raw text though human readable is still unfortunately not yet machine readable. A machine can read in bits and numbers and therefore we will first need to convert our text into numbers for which we utilise a very common approach known as the Bag-of-Words\n",
        "\n",
        "**The Bag of Words approach**\n",
        "\n",
        "This approach uses the counts of words as a starting block and records the occurrence of each word (from the entire text) in a vector specific to that particular word. For example given these two sentences \"I love to eat Burgers\", \"I love to eat Fries\", we first tokenize to obtain our vocabulary of 6 words from which we can get the word counts for - [I, love, to, eat, Burgers, Fries].\n",
        "\n",
        "Vectorizing the text via the Bag of Words approach, we get six distinct vectors one for each word. So you ask since we now have rows consisting of numbers (instead of text) what forms the columns (or features)? Well each word now becomes an individual feature/column in this new transformed dataset. To illustrate this point, I shall utilize the Scikit-learn library to implement a vectorizer that generates a vector of word counts (term frequencies) - via the CountVectorizer method as follows."
      ],
      "metadata": {
        "_uuid": "2ace49576cca7bf5476d2a7e7989e0be76394701",
        "_cell_guid": "fa4f6d1f-eb70-4325-84d1-92212259f98c",
        "id": "U4bdsu7dJJK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining our sentence\n",
        "sentence = ['I love to eat Burgers',\n",
        "            'I love to eat Fries']\n",
        "vectorizer = CountVectorizer(min_df=0)\n",
        "sentence_transform = vectorizer.fit_transform(sentence)"
      ],
      "metadata": {
        "_uuid": "e63eb93cf0e6126d01429f4783981d0afabc6994",
        "_cell_guid": "d6883542-1769-451e-98af-a327db924b93",
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:32.176002Z",
          "iopub.execute_input": "2023-09-11T13:29:32.178816Z",
          "iopub.status.idle": "2023-09-11T13:29:32.191229Z",
          "shell.execute_reply.started": "2023-09-11T13:29:32.17879Z",
          "shell.execute_reply": "2023-09-11T13:29:32.189795Z"
        },
        "trusted": true,
        "id": "yMXVsv-SJJK1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fitting the vectorizer to the dataset**\n",
        "\n",
        "Here we initialize and create a simple term frequency object via the CountVectorizer function simply called \"vectorizer\". The parameters that I have provided explicitly (the rest are left as default) are the bare minimum. Here \"min_df\" in the parameter refers to the minimum document frequency and the vectorizer will simply drop all words that occur less than that value set (either integer or in fraction form).  For a detailed read up on this method as well as the rest of the parameters that one could use, I refer you to the [Sklearn website](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
        "\n",
        "Finally we apply the fit_transform method is actually comprised of two steps. The first step is the fit method where the vectorizer is mapped to the the dataset that you provide. Once this is done,  the actual vectorizing operation is performed via the transform method where the raw text is turned into its vector form as shown below:"
      ],
      "metadata": {
        "_uuid": "0051e63fcc23edad22d610fdf7aee4cf7b91c619",
        "_cell_guid": "7fda51ee-c804-4f42-86f2-836526b3ef23",
        "id": "HAok5URMJJK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The features are:\\n {}'.format(vectorizer.get_feature_names_out()))\n",
        "print('\\nThe vectorized array looks like:\\n {}'.format(sentence_transform.toarray()))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:32.192464Z",
          "iopub.execute_input": "2023-09-11T13:29:32.193042Z",
          "iopub.status.idle": "2023-09-11T13:29:32.209871Z",
          "shell.execute_reply.started": "2023-09-11T13:29:32.192998Z",
          "shell.execute_reply": "2023-09-11T13:29:32.208574Z"
        },
        "trusted": true,
        "id": "Usq-UnCQJJK2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_transform"
      ],
      "metadata": {
        "_uuid": "f86e4bdb8f4e281bad241366637569a9218c182a",
        "_cell_guid": "41bbc164-a059-4cf7-bf22-18973f0200d2",
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:32.211285Z",
          "iopub.execute_input": "2023-09-11T13:29:32.211644Z",
          "iopub.status.idle": "2023-09-11T13:29:32.224574Z",
          "shell.execute_reply.started": "2023-09-11T13:29:32.211615Z",
          "shell.execute_reply": "2023-09-11T13:29:32.223627Z"
        },
        "trusted": true,
        "id": "IvY7Fq-4JJK3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sparse matrix vector ouptuts**\n",
        "\n",
        "From the output of the vectorized text, we can see that the features consist of the words in the corpus of text that we fed into the vectorizer (here the corpus being the two sentences we defined earlier). Simply call the get_feature_names_out attribute from the vectorizer to inspect it.\n",
        "\n",
        "With regards to the transformed text, one would be tempted to inspect the values by simplying calling it. However when you try to call it you really just get a message which states \"*sparse matrix of type class 'numpy.int64' with 8 stored elements in Compressed Sparse Row format*\". Therefore this means that the vectorizer returns the transformed raw text as a matrix where most of its values are zero or almost negligible, hence the term sparse. Thinking about this, it does make sense that our returned matrices contain quite a high degree of sparsity."
      ],
      "metadata": {
        "_uuid": "2fb99a8569606d4c7ab34a17bbb167581a5f560d",
        "_cell_guid": "c3cc1c14-f9f5-46ae-9c40-5a753d66c18c",
        "id": "DOehhMY1JJK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "_uuid": "f648e42baafdead7dee89eb1e031bb9cccd92653",
        "_cell_guid": "6e7b34be-f0bd-4613-809d-5cd545ce9b49",
        "id": "3JtODY2GJJK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Topic modelling\n",
        "\n",
        "![](https://www.hillsboroughcounty.org/library/hillsborough/pumpkin-outside-nr.jpg?h=475&w=845&la=en&hash=B7A332CDADCCCA7909334E874F03A1E5)\n",
        "\n",
        "Arriving at our *Final Destination* (pun intended), I will implement two different topic modelling techniques as follows:\n",
        "\n",
        "1. **Latent Dirichlet Allocation** - Probabilistic, generative model which uncovers the topics latent to a dataset by assigning weights to words in a corpus, where each topic will assign different probability weights to each word.\n",
        "\n",
        "2. **Non-negative Matrix Factorization** - Approximation method that takes an input matrix and approximates the factorization of this matrix into two other matrices, with the caveat that the values in the matrix be non-negative.\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "0b74584921e17f65034fcf5cbe67529cb6b46928",
        "_cell_guid": "cd5d67e8-4e5e-4f6a-8e24-660401fe2403",
        "id": "ZiPqS_i9JJK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a helper function to print top words\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for index, topic in enumerate(model.components_):\n",
        "        message = '\\nTopic #{}:'.format(index)\n",
        "        message += ' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words -1 :-1]])\n",
        "        print(message)\n",
        "        print('='*70)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:32.225438Z",
          "iopub.execute_input": "2023-09-11T13:29:32.225735Z",
          "iopub.status.idle": "2023-09-11T13:29:32.236679Z",
          "shell.execute_reply.started": "2023-09-11T13:29:32.225711Z",
          "shell.execute_reply": "2023-09-11T13:29:32.235743Z"
        },
        "trusted": true,
        "id": "P9-YWnjmJJK6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3a. Putting all the preprocessing steps together\n",
        "\n",
        "This is now the perfect opportunity to piece together all the text preprocessing steps that was mentioned in the previous section. So you're asking yourself, do we really need to go through all the effort and steps again in defining tokenization, stopword removals, stemming/lemmatizing etc?\n",
        "\n",
        "Thankfully we do not need to go through all of that again. I conveniently omitted a key detail about Sklearn vectorizers but will mention it at this juncture. When you vectorize the raw text with CountVectorizer, the dual stages of tokenizing and stopwords filtering are automatically included as a high-level component. Here unlike the NLTK  tokenizer that you were introduced to in the Section 2a earlier, Sklearn's tokenizer discards all single character terms like ('a', 'w' etc) and also lower cases all terms by default. Filtering out stopwords in Sklearn is as convenient as passing the value 'english' into the argument \"stop_words\" where a built-in English stopword list is automatically used.\n",
        "\n",
        "Unfortunately, there is no built-in lemmatizer in the vectorizer so we are left with a couple of options. Either implementing it separately everytime before feeding the data for vectorizing or somehow extend the sklearn implementation to include this functionality. Luckily for us, we have the latter option where we can extend the CountVectorizer class by overwriting the \"build_analyzer\" method as follows:\n",
        "\n",
        "### Extending the CountVectorizer class with a lemmatizer"
      ],
      "metadata": {
        "_uuid": "3c4c26a4a38ad831a826112c0b8823988be164ae",
        "_cell_guid": "951d1106-1277-4719-bf8c-2999a5ddfa66",
        "id": "4XhZ5mHPJJK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemm = WordNetLemmatizer()\n",
        "class LemmaCountVectorizer(CountVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
        "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:32.238026Z",
          "iopub.execute_input": "2023-09-11T13:29:32.238391Z",
          "iopub.status.idle": "2023-09-11T13:29:32.249181Z",
          "shell.execute_reply.started": "2023-09-11T13:29:32.238299Z",
          "shell.execute_reply": "2023-09-11T13:29:32.247872Z"
        },
        "trusted": true,
        "id": "0KIsJlihJJK7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have utilised some subtle concepts from Object-Oriented Programming (OOP). We have essentially inherited and subclassed the original Sklearn's CountVectorizer class and overwritten the build_analyzer method by implementing the lemmatizer for each list in the raw text matrix."
      ],
      "metadata": {
        "_uuid": "2f438bd6e29ac342a201671981ee3343371ae632",
        "_cell_guid": "139ca1ba-dd2b-406e-a845-64c4e2770c3b",
        "id": "ZNLIDxkFJJK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the entire training text in a list\n",
        "text = list(train.text.values)\n",
        "# Calling our overwritten Count vectorizer\n",
        "tf_vectorizer = LemmaCountVectorizer(max_df=0.95,\n",
        "                                    min_df=2,\n",
        "                                    stop_words='english',\n",
        "                                    decode_error='ignore')\n",
        "tf = tf_vectorizer.fit_transform(text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:32.255711Z",
          "iopub.execute_input": "2023-09-11T13:29:32.255998Z",
          "iopub.status.idle": "2023-09-11T13:29:33.751975Z",
          "shell.execute_reply.started": "2023-09-11T13:29:32.255973Z",
          "shell.execute_reply": "2023-09-11T13:29:33.750923Z"
        },
        "trusted": true,
        "id": "svrCO28aJJK8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Revisiting our Term frequencies**\n",
        "\n",
        "Having implemented our lemmatized count vectorizer, let us revist the plots for the term frquencies of the top 50 words (by frequency). As you can see from the plot, all our prior preprocessing efforts have not gone to waste. With the removal of stopwords, the remaining words seem much more meaningful where you can see that all the stopwords in the earlier term frequency plot"
      ],
      "metadata": {
        "_uuid": "c50a9903005de438405ee583ddee2882c5756e68",
        "_cell_guid": "bdde4aac-a975-4429-ae9d-c93ec0692f09",
        "id": "FJutFa7VJJK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = tf_vectorizer.get_feature_names_out()\n",
        "count_vec = np.asarray(tf.sum(axis=0)).ravel()\n",
        "zipped = list(zip(feature_names, count_vec))\n",
        "x, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n",
        "\n",
        "\n",
        "# Plotting the Plot.ly plot for the Top 50 word frequencies\n",
        "data = [go.Bar(x = x[0:50],\n",
        "              y = y[0:50],\n",
        "              marker = dict(colorscale='Jet',\n",
        "                           color = y[0:50]),\n",
        "              text='Word counts')]\n",
        "layout = go.Layout(title='Top 50 word frequencies after preprocessing')\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig, filename='basic-bar')\n",
        "\n",
        "\n",
        "# Plotting the Plot.ly plot for the bottom 100 word frequences\n",
        "data = [go.Bar(x = x[-100:],\n",
        "              y = y[-100:],\n",
        "              marker = dict(colorscale='Portland',\n",
        "                           color=y[-100:]),\n",
        "              text='Word counts')]\n",
        "layout = go.Layout(title='Bottom 100 word frequences after preprocessing')\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig, filename='basic-bar')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:33.753355Z",
          "iopub.execute_input": "2023-09-11T13:29:33.753731Z",
          "iopub.status.idle": "2023-09-11T13:29:33.842798Z",
          "shell.execute_reply.started": "2023-09-11T13:29:33.753701Z",
          "shell.execute_reply": "2023-09-11T13:29:33.84193Z"
        },
        "trusted": true,
        "id": "eQTzW-QbJJK9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3b. Latent Dirichlet Allocation\n",
        "\n",
        "Finally we arrive on the subject of topic modelling and the implementation of a couple of unsupervised learning algorithms. The first method that I will touch upon is [Latent Dirichlet Allocation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). Now there are a couple of different implements of this LDA algorithm but in this notebook, I will be using Sklearn's implementation. Another very well-known LDA implementation is Radim Rehurek's [gensim](https://radimrehurek.com/gensim/), so check it out as well."
      ],
      "metadata": {
        "_uuid": "5189dc27414c9b89d26bb1255075d91ae11b47ac",
        "_cell_guid": "fd387f8f-cf4c-4a4f-bfce-c22af2c7e69a",
        "id": "gLJsfZxOJJK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Corpus - Document - Word : Topic Generation**\n",
        "\n",
        "In LDA, the modelling process revolves around three things: the text corpus, its collection of documents, D and the words W in the documents. Therefore the algorithm attempts to uncover K topics from this corpus via the following way (illustrated by the diagram)\n",
        "\n",
        "![Three_Level Bayesian Model](http://scikit-learn.org/stable/_images/lda_model_graph.png)\n",
        "\n",
        "* Model each topic, $\\kappa$ via a Dirichlet prior distribution given by $\\beta_{k}$\n",
        "\n",
        "\n",
        "* Model each document d by another Dirichlet distribution parameterized by $\\alpha$\n",
        "\n",
        "\n",
        "* Subsequently for document d, we generate a topic via a multinomial distribution which we then backtrack and use to generate the correspondings words related to that topic via another multinomial distribution\n",
        "\n",
        "\n",
        "The LDA algorithm first models documents via a mixture model of topics. From these topics, words are then assigned weights based on the probability distribution of these topics. It is this probabilistic assignment over words that allow a user of LDA to say how likely a particular word falls into a topic. Subsequently from the collection of words assigned to a particular topic, are we thus able to gain an insight as to what that topic may actually represent from a lexical point of view.\n",
        "\n",
        "From a standard LDA model, there are really a few key parameters that we have to keep in mind and consider programmatically tuning before we invoke the model:\n",
        "1. n_components: The number of topics that you specify to the model\n",
        "2. $\\alpha$ parameter: This is the dirichlet parameter that can be linked to the document topic prior\n",
        "3. $\\beta$ parameter: This is the dirichlet parameter linked to the topic word prior\n",
        "\n",
        "To invoke the  algorithm, we simply create an LDA instance through the Sklearn's *LatentDirichletAllocation* function. The various parameters would ideally have been obtained through some sort of validation scheme. In this instance, the optimal value of n_components (or topic number) was found by conducting a KMeans + Latent Semantic Analysis Scheme whereby the number of Kmeans clusters and number of LSA dimensions were iterated through and the best silhouette mean score."
      ],
      "metadata": {
        "_uuid": "1bde06819b79a77474cff57b4082c451c7649ef8",
        "_cell_guid": "5e9aae62-cfe9-41fc-89f7-db7bc52856d3",
        "id": "VXC-ROkBJJK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LatentDirichletAllocation(n_components=11, max_iter=5, learning_method='online', learning_offset=50, random_state=17)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:33.844165Z",
          "iopub.execute_input": "2023-09-11T13:29:33.844458Z",
          "iopub.status.idle": "2023-09-11T13:29:33.849633Z",
          "shell.execute_reply.started": "2023-09-11T13:29:33.844435Z",
          "shell.execute_reply": "2023-09-11T13:29:33.848498Z"
        },
        "trusted": true,
        "id": "2Org_Ww-JJLA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "lda.fit(tf)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:33.851117Z",
          "iopub.execute_input": "2023-09-11T13:29:33.85136Z",
          "iopub.status.idle": "2023-09-11T13:29:46.167042Z",
          "shell.execute_reply.started": "2023-09-11T13:29:33.851335Z",
          "shell.execute_reply": "2023-09-11T13:29:46.166374Z"
        },
        "trusted": true,
        "id": "AzzKqtJjJJLB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topics generated by LDA\n",
        "\n",
        "We will utilise our helper function we defined earlier \"print_top_words\" to return the top 10 words attributed to each of the LDA generated topics. To select the number of topics, this is handled through the parameter n_components in the function."
      ],
      "metadata": {
        "_uuid": "33dba856c6a1bf17c8beeff6873502083cecfa65",
        "_cell_guid": "83b4227f-9bca-4a42-8038-91d57c75bd38",
        "id": "auuHkAvrJJLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_top_words = 10\n",
        "print('\\nTopics in LDA model: ')\n",
        "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "print_top_words(lda, tf_feature_names, n_top_words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:46.168043Z",
          "iopub.execute_input": "2023-09-11T13:29:46.168617Z",
          "iopub.status.idle": "2023-09-11T13:29:46.197016Z",
          "shell.execute_reply.started": "2023-09-11T13:29:46.16859Z",
          "shell.execute_reply": "2023-09-11T13:29:46.195031Z"
        },
        "trusted": true,
        "id": "DG2G-LrdJJLB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "first_topic = lda.components_[0]\n",
        "second_topic = lda.components_[1]\n",
        "third_topic = lda.components_[2]\n",
        "fourth_topic = lda.components_[3]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:46.198258Z",
          "iopub.execute_input": "2023-09-11T13:29:46.198858Z",
          "iopub.status.idle": "2023-09-11T13:29:46.331242Z",
          "shell.execute_reply.started": "2023-09-11T13:29:46.198831Z",
          "shell.execute_reply": "2023-09-11T13:29:46.330283Z"
        },
        "trusted": true,
        "id": "pvt1e34gJJLC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "first_topic.shape"
      ],
      "metadata": {
        "_uuid": "dd631641530a698e2c22cfb222da9da223888c75",
        "_cell_guid": "5b0cbf55-cc66-4b06-9578-da04affe5d8b",
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:46.334089Z",
          "iopub.execute_input": "2023-09-11T13:29:46.334431Z",
          "iopub.status.idle": "2023-09-11T13:29:46.343901Z",
          "shell.execute_reply.started": "2023-09-11T13:29:46.334399Z",
          "shell.execute_reply": "2023-09-11T13:29:46.342754Z"
        },
        "trusted": true,
        "id": "rOypB0NnJJLD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Cloud visualizations of the topics"
      ],
      "metadata": {
        "_uuid": "982f80f378c5c5179b1b9392a19a0d8544816220",
        "_cell_guid": "6eb1a021-f48d-47fc-8917-e8ab62aabf2d",
        "id": "gsvyp9gVJJLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 -1 : -1]]\n",
        "second_topic_words = [tf_feature_names[i] for i in second_topic.argsort()[:-50 -1: -1]]\n",
        "third_topic_words = [tf_feature_names[i] for i in third_topic.argsort()[:-50 -1: -1]]\n",
        "fourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 -1: -1]]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:46.345042Z",
          "iopub.execute_input": "2023-09-11T13:29:46.345889Z",
          "iopub.status.idle": "2023-09-11T13:29:46.35957Z",
          "shell.execute_reply.started": "2023-09-11T13:29:46.345818Z",
          "shell.execute_reply": "2023-09-11T13:29:46.358373Z"
        },
        "trusted": true,
        "id": "PMLfqPZmJJLD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word cloud of First Topic"
      ],
      "metadata": {
        "_uuid": "d51bee99ddbfa150ce54d55a73284638bbffeb10",
        "_cell_guid": "80904ac8-1724-4853-b416-f07210021b35",
        "id": "R-TSDkVjJJLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the wordcloud with the values under the category dataframe\n",
        "firstcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=1800).generate(' '.join(first_topic_words))\n",
        "plt.imshow(firstcloud)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:46.360918Z",
          "iopub.execute_input": "2023-09-11T13:29:46.361998Z",
          "iopub.status.idle": "2023-09-11T13:29:51.832494Z",
          "shell.execute_reply.started": "2023-09-11T13:29:46.361966Z",
          "shell.execute_reply": "2023-09-11T13:29:51.831277Z"
        },
        "trusted": true,
        "id": "KDgAye1VJJLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the wordcloud with the values under the category dataframe\n",
        "cloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=1800).generate(' '.join(second_topic_words))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:51.833982Z",
          "iopub.execute_input": "2023-09-11T13:29:51.834356Z",
          "iopub.status.idle": "2023-09-11T13:29:56.215047Z",
          "shell.execute_reply.started": "2023-09-11T13:29:51.834323Z",
          "shell.execute_reply": "2023-09-11T13:29:56.213775Z"
        },
        "trusted": true,
        "id": "eMdx58-AJJLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the wordcloud with the values under the category dataframe\n",
        "cloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=1800).generate(' '.join(third_topic_words))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:56.216426Z",
          "iopub.execute_input": "2023-09-11T13:29:56.216772Z",
          "iopub.status.idle": "2023-09-11T13:30:01.29075Z",
          "shell.execute_reply.started": "2023-09-11T13:29:56.216746Z",
          "shell.execute_reply": "2023-09-11T13:30:01.289333Z"
        },
        "trusted": true,
        "id": "kggKbwwcJJLG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the wordcloud with the values under the category dataframe\n",
        "cloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=1800).generate(' '.join(fourth_topic_words))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:30:01.292164Z",
          "iopub.execute_input": "2023-09-11T13:30:01.292943Z",
          "iopub.status.idle": "2023-09-11T13:30:05.930234Z",
          "shell.execute_reply.started": "2023-09-11T13:30:01.292913Z",
          "shell.execute_reply": "2023-09-11T13:30:05.928401Z"
        },
        "trusted": true,
        "id": "4_udiEagJJLG"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}